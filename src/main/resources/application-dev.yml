# 开发环境配置
server:
  # 服务器的HTTP端口，默认为80
  port: 8088

spring:
  datasource:
    druid:
      driver-class-name: com.mysql.cj.jdbc.Driver
      url: jdbc:mysql://localhost:3306/springboot-pro?zeroDateTimeBehavior=convertToNull&useUnicode=true&characterEncoding=UTF-8&autoReconnect=true
      username: root
      password: 123456
      #druid配置
      #配置初始化大小/最小/最大
      initial-size: 5
      min-idle: 5
      max-active: 20
      #获取连接等待超时时间
      max-wait: 60000
      #间隔多久进行一次检测，检测需要关闭的空闲连接
      time-between-eviction-runs-millis: 60000

  servlet:
    #文件上传相关配置
    multipart:
      # 开启 multipart 上传功能
      enabled: true
      #文件上传路径
      location: E:/pro/file
      # 文件写入磁盘的阈值
      file-size-threshold: 2KB
      # 最大文件大小
      max-file-size: 200MB
      # 最大请求大小
      max-request-size: 215MB


  # 整合ES配置 本地ES安装路径 D:\SDK\elasticsearch\elasticsearch-7.13.2
  elasticsearch:
    rest:
      uris: http://localhost:9200

# Redis配置  本地Redis安装路径 D:\SDK\redis
  redis:
    host: 127.0.0.1
    port: 6379
    password:
    lettuce:
      pool:
        max-active: 1000  #连接池最大连接数（使用负值表示没有限制）
        max-wait: -1ms  #连接池最大阻塞等待时间（使用负值表示没有限制）
        max-idle: 10  #连接池中的最大空闲连接
        min-idle: 0   #连接池中的最小空闲连接
      shutdown-timeout: 6000ms  #连接超时时间（毫秒）

#Kafka配置 本地kafka安装路径，D:\SDK\kafka\kafka_2.13-2.8.0
#启动zookeeper D:\SDK\zookeeper\zookeeper-3.4.11\bin  zkServer.cmd
#使用配置文件启动kafka windows目录 启动命令：kafka-server-start.bat ..\..\config\server.properties

  kafka:
    bootstrap-servers: 127.0.0.1:9092,127.0.0.1:9093,127.0.0.1:9094 # kafka集群信息
    producer: # 生产者配置
      retries: 3 # 设置大于0的值，则客户端会将发送失败的记录重新发送
      batch-size: 16384 #16K
      buffer-memory: 33554432 #32M
      acks: 1
      # 指定消息key和消息体的编解码方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      group-id: zhTestGroup # 消费者组
      enable-auto-commit: false # 关闭自动提交
      auto-offset-reset: earliest # 当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    listener:
      # 当每一条记录被消费者监听器（ListenerConsumer）处理之后提交
      # RECORD
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后提交
      # BATCH
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间大于TIME时提交
      # TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后，被处理record数量大于等于COUNT时提交
      # COUNT
      # TIME |　COUNT　有一个条件满足时提交
      # COUNT_TIME
      # 当每一批poll()的数据被消费者监听器（ListenerConsumer）处理之后, 手动调用Acknowledgment.acknowledge()后提交
      # MANUAL
      # 手动调用Acknowledgment.acknowledge()后立即提交，一般使用这种
      # MANUAL_IMMEDIATE
      ack-mode: manual_immediate

# 定时任务配置
scheduled:
  # 每隔1分钟执行一次  【秒 分 时 日 月 周】
  cronTime: 0 */1 * * * ?
